{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Subset\n\nimport os\nimport PIL.Image as Image\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-01-15T02:36:28.826974Z","iopub.execute_input":"2022-01-15T02:36:28.827474Z","iopub.status.idle":"2022-01-15T02:36:30.481866Z","shell.execute_reply.started":"2022-01-15T02:36:28.827391Z","shell.execute_reply":"2022-01-15T02:36:30.481151Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-01-15T02:36:30.484874Z","iopub.execute_input":"2022-01-15T02:36:30.485392Z","iopub.status.idle":"2022-01-15T02:36:30.539436Z","shell.execute_reply.started":"2022-01-15T02:36:30.485353Z","shell.execute_reply":"2022-01-15T02:36:30.538617Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# check data csv files\n\ndata_dir = '../input/petfinder-pawpularity-score'\n\ndef get_dataframe(data_dir, is_train=True):\n    \n    if is_train:\n        image_dir = os.path.join(data_dir, 'train')\n        file_path = os.path.join(data_dir, 'train.csv')\n    else:\n        image_dir = os.path.join(data_dir, 'test')\n        file_path = os.path.join(data_dir, 'test.csv')\n    \n    df = pd.read_csv(file_path)\n\n    # set image filepath\n    df['img_file_path'] = df['Id'].apply(lambda x: os.path.join(image_dir, f'{x}.jpg'))\n    \n    return df\n\ntrain_df = get_dataframe(data_dir, is_train=True)\ntest_df = get_dataframe(data_dir, is_train=False)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T02:36:30.542826Z","iopub.execute_input":"2022-01-15T02:36:30.543038Z","iopub.status.idle":"2022-01-15T02:36:30.630517Z","shell.execute_reply.started":"2022-01-15T02:36:30.543012Z","shell.execute_reply":"2022-01-15T02:36:30.628880Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# define custom dataset for pytorch\n\nclass PawpularityDataset(Dataset):\n    def __init__(self, image_filepaths, covariates, targets, transform):\n        self.image_filepaths = image_filepaths\n        self.targets = targets\n        self.transform = transform\n        self.covaraites_all = covariates\n    \n    def __len__(self):\n        return len(self.image_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.image_filepaths[idx]\n        covaraites_per_image = torch.tensor(self.covaraites_all[idx])\n        target = torch.tensor(self.targets[idx])\n        \n        with open(image_filepath, 'rb') as f:\n            image = Image.open(f)\n            image = image.convert('RGB')\n            image = self.transform(image)\n        \n        return image, covaraites_per_image, target","metadata":{"_uuid":"090fadb4-d98d-4004-8ab7-a19e1e323550","_cell_guid":"d27eebed-ef87-4173-8a1d-aff83e794fe1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-15T02:36:30.632445Z","iopub.execute_input":"2022-01-15T02:36:30.632983Z","iopub.status.idle":"2022-01-15T02:36:30.640089Z","shell.execute_reply.started":"2022-01-15T02:36:30.632945Z","shell.execute_reply":"2022-01-15T02:36:30.639077Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# dataloader\n\ndef load_data(data_dir, batch_size=32, is_train=True, use_subset=False):\n    \"\"\"\n    return the train dataloader\n    \"\"\"\n    \n    # images and targets\n    if is_train:\n        df = get_dataframe(data_dir, is_train=True)\n        images = df['img_file_path'].to_numpy()\n        targets = df['Pawpularity'].to_numpy()\n    else:\n        df = get_dataframe(data_dir, is_train=False)\n        images = df['img_file_path'].to_numpy()\n        targets = np.zeros_like(images)\n    \n    # covariates [2:13]\n    # But here for computational complexity we will only choose a few\n    selected_columns = ['Accessory', 'Collage', 'Human']\n    covariates = df.loc[:, selected_columns].to_numpy()\n    \n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)\n    ])\n    \n    dataset = PawpularityDataset(image_filepaths=images, covariates=covariates, targets=targets, transform=transform)\n    \n    subset_ind = list(range(500))\n    \n    data_subset = Subset(dataset, subset_ind)\n\n    # data loader\n    data_loader = DataLoader(dataset=data_subset if use_subset else dataset, \n                                batch_size=batch_size,\n                                shuffle=True)\n    \n    return data_loader\n\n\ntrain_loader = load_data(data_dir, is_train=True, use_subset=False)\ntest_loader = load_data(data_dir, is_train=False)","metadata":{"_uuid":"6aacd1fb-9f9c-496e-bb04-331f2bf1d703","_cell_guid":"406faa10-05b8-41c6-9b80-f571df073e23","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-15T02:36:30.641646Z","iopub.execute_input":"2022-01-15T02:36:30.642533Z","iopub.status.idle":"2022-01-15T02:36:30.701280Z","shell.execute_reply.started":"2022-01-15T02:36:30.642434Z","shell.execute_reply":"2022-01-15T02:36:30.700563Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# VGG 16 baseline model\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000, init_weights=True): # change to binary classifier\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfg_vgg16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n\n# this is pretrained weights on ImageNet\nmodel_path = {\n    'vgg16_bn': '../input/vgg16-pretrained-models/vgg16_bn_pretrained.pth'\n}\n\ndef _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n    if pretrained:\n        kwargs['init_weights'] = False\n    model = VGG(make_layers(cfg, batch_norm=batch_norm), **kwargs)\n    if pretrained:\n        state_dict = torch.load(model_path[arch])\n        model.load_state_dict(state_dict)\n    return model\n\ndef vgg16_bn(pretrained=False, progress=True, **kwargs):\n    \"\"\" (CUSTOMIZED) VGG 16-layer model (configuration \"D\") with batch normalization\n    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _vgg('vgg16_bn', cfg_vgg16, True, pretrained, progress, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T02:36:30.702584Z","iopub.execute_input":"2022-01-15T02:36:30.702947Z","iopub.status.idle":"2022-01-15T02:36:30.721131Z","shell.execute_reply.started":"2022-01-15T02:36:30.702910Z","shell.execute_reply":"2022-01-15T02:36:30.720164Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# VGG 16 Hybrid Model\n\n#########################\n# The hybrid Conv2d layer\n#########################\n\nclass Hybrid_Conv2d(nn.Module):\n    \"\"\"    \n    (self, channel_in, channel_out, kernel_size, cov, stride=1, padding=0)\n    kernel_size are 4d weights: (out_channel, in_channel, height, width)\n    \"\"\"    \n    def __init__(self, channel_in, channel_out, kernel_size, num_cov, stride=1, padding=0):\n        super(Hybrid_Conv2d, self).__init__()\n        self.kernel_size = kernel_size # 4D weight (out_channel, in_channel, height, width)\n        self.channel_in = channel_in\n        self.channel_out = channel_out\n        self.stride = stride\n        self.padding = padding\n        self.num_cov = num_cov # number of covariates\n\n        self.W_0 = nn.Parameter(torch.randn(kernel_size), requires_grad=True)\n        self.W = []\n        for r in range(self.num_cov):\n            W_r = nn.Parameter(torch.randn(kernel_size), requires_grad=True)\n            self.W.append(W_r)        \n        \n        self._initialize_weights()\n        \n    # weight initialization\n    def _initialize_weights(self):\n        nn.init.kaiming_normal_(self.W_0, mode='fan_out', nonlinearity='relu')\n        for r in range(self.num_cov):\n            nn.init.kaiming_normal_(self.W[r], mode='fan_out', nonlinearity='relu')\n \n    def forward(self, x, cov):\n        # input x is of shape = (batchsize, channel=3, width, height) e.g. (32, 3, 224, 224)\n        # cov: 2d tensor of shape (batchsize, r): \n        # r = number of covariates per image; \n        # bs = batchsize = number of images\n        \n        outputs = []\n        for i in range(cov.shape[0]): # for every image x[i] there are r covariates\n            res = torch.zeros_like(self.W_0)\n            self.W_0 = self.W_0.to('cuda:0')\n            for j in range(cov.shape[1]): # for every cov\n                self.W[j] = self.W[j].to('cuda:0')\n                res = res.to('cuda:0')\n                res = res + ( torch.mul(self.W[j], cov[i][j]) ).to('cuda:0') # cov[i] is an array with shape (r,); cov[i][j] is either 1 or 0\n            \n            kernel = self.W_0 + res\n            x_i = torch.unsqueeze(x[i], 0) # (3, 224, 224) -> (1, 3, 224, 224) for 4d weight shape matching\n            out = F.conv2d(x_i, kernel, stride=self.stride, padding=self.padding)\n            outputs.append(out) \n            \n        outputs = torch.cat(outputs)\n        return outputs\n\nclass HybridVGG16(nn.Module):\n    \"\"\"\n    Hybrid Vgg16_bn network: A pretrained vgg16_bn with FIRST conv layer being a Hybrid_Conv2d layer\n    \"\"\"\n    def __init__(self):\n        super(HybridVGG16, self).__init__()\n        # load pytorch vgg16 with pretrained weights\n        vgg = vgg16_bn(pretrained=True)\n\n        # set the three blocks you need for forward pass\n        # remove the first conv layer + relu from the feature extractor\n        self.features = vgg.features[1:]\n        self.avgpool = vgg.avgpool\n        self.classifier = vgg.classifier\n        \n        # hybrid layers\n        self.hybrid_conv = Hybrid_Conv2d(3, 64, kernel_size=(64, 3, 3, 3), num_cov=3) \n        \n    # Set your own forward pass\n    def forward(self, x, cov):\n        x = self.hybrid_conv(x, cov)\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-15T02:37:15.593854Z","iopub.execute_input":"2022-01-15T02:37:15.594099Z","iopub.status.idle":"2022-01-15T02:37:15.610928Z","shell.execute_reply.started":"2022-01-15T02:37:15.594072Z","shell.execute_reply":"2022-01-15T02:37:15.610263Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained model (skip train)\n# note: no need for loss function and optimizer anymore\n\n# evaluate\ndef evaluate(test_loader, data_dir, model_name, device):\n    \n    if model_name == \"baseline\":\n        model = vgg16_bn(pretrained=False).to(device) # this \"False\" is w.r.t. ImageNet pretrained weights\n        checkpoint_path = '../input/vgg16-pretrained-models/vgg16_baseline_checkpoint.ckpt'\n    elif model_name == \"hybrid\":\n        model = HybridVGG16().to(device) \n        checkpoint_path = '../input/vgg16-pretrained-models/vgg16_hybrid.ckpt'\n        \n    if isinstance(model, VGG) or isinstance(model, HybridVGG16):\n        num_ftrs = model.classifier[6].in_features\n        model.classifier[6] = nn.Linear(num_ftrs, 1)\n\n    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n\n    model.eval() \n    \n    print('Making predictions...')\n    \n    test_pred = []    \n    test_df = get_dataframe(data_dir, is_train=False)\n    \n    with torch.no_grad():\n        for (test_images, covariates, test_labels) in tqdm(test_loader):\n            test_images = test_images.to(device).float()\n            covariates = covariates.to(device).float()\n            test_labels = test_labels.to(device).float()\n            # forward pass\n            if isinstance(model, VGG):\n                outputs = model(test_images)               # baseline vgg\n            else:\n                outputs = model(test_images, covariates)    # hybrid model takes covariate here\n  \n            test_pred.extend(outputs.cpu().detach().squeeze().numpy().tolist())\n            \n\n        # write to file\n        output_df = pd.DataFrame({\"Id\": test_df['Id'], \"Pawpularity\": test_pred})\n        \n        # check output\n        # output_df = pd.read_csv('submission.csv')\n\n        return output_df\n\n\nmodel_name = \"hybrid\"\noutput_df = evaluate(test_loader, data_dir, model_name, device) ","metadata":{"execution":{"iopub.status.busy":"2022-01-15T02:37:16.175929Z","iopub.execute_input":"2022-01-15T02:37:16.176434Z","iopub.status.idle":"2022-01-15T02:37:35.917586Z","shell.execute_reply.started":"2022-01-15T02:37:16.176398Z","shell.execute_reply":"2022-01-15T02:37:35.916431Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"output_df.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T02:37:35.918611Z","iopub.status.idle":"2022-01-15T02:37:35.919408Z","shell.execute_reply.started":"2022-01-15T02:37:35.919136Z","shell.execute_reply":"2022-01-15T02:37:35.919162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}